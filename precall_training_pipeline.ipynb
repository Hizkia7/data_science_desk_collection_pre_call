{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiGTqtF4nBnZ"
   },
   "outputs": [],
   "source": [
    "# Specify the Google Cloud region where the services are located\n",
    "REGION = \"asia-southeast2\"\n",
    "\n",
    "# Define the service account email used for Google Cloud services authentication\n",
    "SERVICE_ACCOUNT = 'compute@developer.gserviceaccount.com'\n",
    "\n",
    "# Provide the service account's unique identifier number\n",
    "SERVICE_ACCOUNT_NUMBER = '1182'\n",
    "\n",
    "# Execute the command to get the current Google Cloud project ID\n",
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "\n",
    "# Print the retrieved project ID\n",
    "print(\"Current Google Cloud Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIkE6-zanGSU"
   },
   "outputs": [],
   "source": [
    "# Define the client name associated with the operations\n",
    "CLIENT_NAME = \"client\"\n",
    "\n",
    "# Specify the URI of the Google Cloud Storage bucket\n",
    "BUCKET_URI = 'gs://dev-bucket-ds-dashboard'\n",
    "\n",
    "# Extract and define the bucket name from the bucket URI\n",
    "BUCKET_NAME = BUCKET_URI\n",
    "\n",
    "# Define the root directory for pipelines within the bucket, specific to the client\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/data_science/{CLIENT_NAME}\"\n",
    "\n",
    "# Define the full Google Cloud Storage bucket path for storing temporary data\n",
    "GCS_BUCKET_NAME = 'gs://dev-bucket-ds-dashboard/data_science/data_vertex/'\n",
    "\n",
    "# Define the full path to the dataset within the GCS bucket\n",
    "GCS_PATH = 'gs://dev-bucket-ds-dashboard/data_science/data_vertex/dataset.csv'\n",
    "\n",
    "# Specify the format of the training dataset\n",
    "TRAINING_DATASET_FORMAT = \"csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-5LBV6VnGZz"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.v1.custom_job import utils\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component, OutputPath, ClassificationMetrics\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from typing import NamedTuple\n",
    "import pandas as pd\n",
    "import google.cloud.bigquery as bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drg6xIM0nGcU"
   },
   "outputs": [],
   "source": [
    "DEPLOY_IMAGE = \"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aRd9F5unGe6"
   },
   "outputs": [],
   "source": [
    "# Define the base path for all project and location based operations\n",
    "PARENT = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "\n",
    "# Configure the API endpoint specific to the region for AI Platform\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "\n",
    "# Set client options with the specific API endpoint\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "# Initialize the AI Platform Job Service Client with the specified endpoint\n",
    "CLIENT = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "\n",
    "# Print the API endpoint used for the client to verify the configuration\n",
    "print(\"AI Platform API Endpoint:\", API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T629iizAnGhS"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery[pandas]==3.10.0\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"pandas\",\n",
    "        \"joblib\",\n",
    "        \"scikit-learn\",\n",
    "    ],\n",
    "    base_image = 'python:3.9'\n",
    ")\n",
    "def get_and_preprocess_dataset(\n",
    "    project_id : str,\n",
    "    dataset: Output[Dataset],\n",
    "    client_name: str\n",
    ") :\n",
    "    \"\"\"\n",
    "    Retrieves and preprocesses a dataset from BigQuery based on the project and client details,\n",
    "    then applies encoding and dumps to a CSV.\n",
    "\n",
    "    Parameters:\n",
    "        project_id (str): The Google Cloud project ID.\n",
    "        dataset (Output[Dataset]): The dataset to be processed and stored.\n",
    "        client_name (str): The client name to filter the data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    def ensure_min_dpd(df):\n",
    "        df.loc[df['dpd_number'] < 90, 'dpd_number'] = 90\n",
    "        return df\n",
    "\n",
    "    def remove_zero_principal(df):\n",
    "        return df[df['outstanding_principal_amount'] != 0]\n",
    "\n",
    "    def create_employment_dummies(df):\n",
    "        employment_types = ['EMPLOYEE', 'ENTREPRENEUR', 'STUDENT', 'OTHERS', 'FREELANCE',\n",
    "                            'GOVERNMENT EMPLOYEE', 'POLICE/MILITARY', 'UNEMPLOYED', 'UNKNOWN']\n",
    "        df_employment = pd.get_dummies(df['employment_type_name'])\n",
    "        df_employment = df_employment.reindex(columns=employment_types, fill_value=0)\n",
    "        df_employment.columns = [f'employment_type_{col}' for col in df_employment.columns]\n",
    "        return df_employment\n",
    "\n",
    "    def create_gender_dummies(df):\n",
    "        gender = ['M', 'F']\n",
    "        df_gender = pd.get_dummies(df['gender_name'])\n",
    "        df_gender = df_gender.reindex(columns=gender, fill_value=0)\n",
    "        df_gender.columns = [f'gender_name_{col}' for col in df_gender.columns]\n",
    "        return df_gender\n",
    "\n",
    "    def concatenate_dataframes(df, df_employment, df_gender):\n",
    "        return pd.concat([df, df_employment, df_gender], axis=1)\n",
    "\n",
    "    def drop_unnecessary_columns(df):\n",
    "        drop_columns = ['external_loan_number', 'ktp_province_name', 'total_payment', 'age',\n",
    "                        'campaign_valid_from', 'campaign_valid_to', 'campaign_name', 'client_name',\n",
    "                        'employment_type_name', 'gender_name', 'Ever_Pay']\n",
    "        return df.drop(columns=drop_columns)\n",
    "\n",
    "    def apply_ordinal_encoding(df, cols):\n",
    "        encoder = OrdinalEncoder()\n",
    "        df[cols] = encoder.fit_transform(df[cols])\n",
    "        return df\n",
    "\n",
    "\n",
    "    # Get dataset from BigQuery\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `pav-id-vertexai-dev.data_science.ds_pre_call`\n",
    "    WHERE client_name = \"{0}\"\n",
    "    LIMIT 100\n",
    "    \"\"\".format(client_name)\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query=query, job_config=job_config)\n",
    "    df = query_job.result().to_dataframe()\n",
    "\n",
    "    # Apply functions\n",
    "    df = ensure_min_dpd(df)\n",
    "    df = remove_zero_principal(df)\n",
    "    df_employment = create_employment_dummies(df)\n",
    "    df_gender = create_gender_dummies(df)\n",
    "    df = concatenate_dataframes(df, df_employment, df_gender)\n",
    "    df = drop_unnecessary_columns(df)\n",
    "    df = apply_ordinal_encoding(df, ['dpd_number', 'outstanding_principal_amount'])\n",
    "\n",
    "    df.to_csv(dataset.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7jhGxITnGjr"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "        \"joblib\",\n",
    "        \"scikit-learn\",\n",
    "        \"xgboost\",\n",
    "    ],\n",
    "    base_image = 'python:3.9'\n",
    ")\n",
    "def model_training(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    metrics_clf: Output[ClassificationMetrics],\n",
    "    client_name: str\n",
    "):\n",
    "\n",
    "    import os\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import (accuracy_score, precision_recall_curve,\n",
    "                                 roc_auc_score, f1_score, precision_score,\n",
    "                                 recall_score, confusion_matrix, make_scorer)\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "    # Load the training census dataset\n",
    "    with open(dataset.path, \"r\") as train_data:\n",
    "        raw_data = pd.read_csv(train_data)\n",
    "\n",
    "    # Step 4: Splitting data into features (X) and target (y)\n",
    "    X = raw_data.drop(columns=['Ever_Pay'])\n",
    "    y = raw_data['Ever_Pay']\n",
    "    y=y.astype('int')\n",
    "\n",
    "\n",
    "    # Step 5: Splitting data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y,\n",
    "        random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Custom scoring function\n",
    "    def custom_scorer(y_true, y_pred):\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        return f1 if precision > recall else 0\n",
    "\n",
    "    # Create the scorer\n",
    "    scorer = make_scorer(custom_scorer, greater_is_better=True)\n",
    "\n",
    "    # Define the model\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "    # Define the hyperparameter grid for scale_pos_weight\n",
    "    param_grid = {\n",
    "        'scale_pos_weight': np.arange(0,1)\n",
    "    }\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring=scorer, cv=5)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    #Create Prediction\n",
    "    predictions = grid_search.predict(X_test)\n",
    "\n",
    "\n",
    "    # Menghitung skor akurasi, AUC, dan kurva presisi-recall\n",
    "    score = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, pos_label=1, average='binary')\n",
    "    precision = precision_score(y_test, predictions, pos_label=1, average='binary')\n",
    "    recall = recall_score(y_test, predictions, pos_label=1, average='binary')\n",
    "    _ = precision_recall_curve(y_test, predictions)\n",
    "\n",
    "    # Log metrics\n",
    "    metrics.log_metric(\"accuracy\", (score * 100.0))\n",
    "    metrics.log_metric(\"framework\", \"XGBoost\")\n",
    "    metrics.log_metric(\"F1-Score\", f1)\n",
    "    metrics.log_metric(\"precision\", precision)\n",
    "    metrics.log_metric(\"recall\", recall)\n",
    "    metrics.log_metric(\"dataset_size\", len(raw_data))\n",
    "\n",
    "    # Export the model to a file\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "    model_file_path = os.path.join(model.path, 'model.joblib')\n",
    "    joblib.dump(grid_search.best_estimator_, model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5k5-iLsgnGl0"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"Pre-Call-model\"\n",
    "PREBUILT_CONTAINER = \"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "GCS_PATH = 'gs://dev-bucket-ds-dashboard/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sonZVscHnGn-"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform==1.25.0\"],\n",
    "    base_image = 'python:3.9'\n",
    ")\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    # gcs_path: str,\n",
    "    region: str,\n",
    "    vertex_model: Output[Model],\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    client_name : str\n",
    "):\n",
    "    \"\"\"\n",
    "    Deploys a machine learning model to Google Cloud Vertex AI.\n",
    "\n",
    "    Parameters:\n",
    "        - model: The model to be deployed.\n",
    "        - project_id: Google Cloud project ID.\n",
    "        - model_name: Display name of the model in Vertex AI.\n",
    "        - region: Google Cloud region to deploy the model.\n",
    "        - vertex_model: Output artifact representing the deployed model.\n",
    "        - vertex_endpoint: Output artifact representing the deployment endpoint.\n",
    "        - client_name: Name of the client, used for additional context (currently not used in deployment).\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    listed_model = aiplatform.Model.list( # GET ALL MODEL IN MODEL REGISTRY , FILTERED BY DISPLAY NAME\n",
    "        filter= 'display_name = \"{}\"'.format(model_name),\n",
    "        project=project_id,\n",
    "        location=region\n",
    "    )\n",
    "\n",
    "    if len(listed_model) > 0: #IF MODEL EXISTS IN REGISTRY\n",
    "        model_version = listed_model[0]\n",
    "        deployed_model = aiplatform.Model.upload(\n",
    "            display_name= model_name,\n",
    "            parent_model=model_version.resource_name,\n",
    "            artifact_uri = model.uri,\n",
    "            serving_container_image_uri = \"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "            location = region,\n",
    "        )\n",
    "    else: #IF MODEL IS NEW IN REGISTRY\n",
    "         deployed_model = aiplatform.Model.upload(\n",
    "            display_name = model_name,\n",
    "            artifact_uri = model.uri,\n",
    "            serving_container_image_uri = \"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "            location=region,\n",
    "        )\n",
    "\n",
    "    # Deploy the model to an endpoint\n",
    "#     endpoint = deployed_model.deploy(machine_type=\"e2-standard-4\")\n",
    "\n",
    "#     # Set the URIs for the deployed model and its endpoint in the output artifacts\n",
    "#     vertex_endpoint.uri = endpoint.resource_name\n",
    "#     vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt1FfLfCneYj"
   },
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"precall-training-pipeline\",\n",
    "    pipeline_root = PIPELINE_ROOT\n",
    ")\n",
    "def pre_call_pipeline(\n",
    "    project_id: str = PROJECT_ID ,\n",
    "    bucket_name: str = BUCKET_NAME,\n",
    "    bucket_uri: str = BUCKET_URI,\n",
    "    client_name: str = CLIENT_NAME,\n",
    "    region: str = REGION\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline for training and deploying a model for pre-call analysis.\n",
    "    This pipeline handles data retrieval, model training, and model deployment.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): Google Cloud Project ID where the services are hosted.\n",
    "        bucket_name (str): Name of the Google Cloud Storage bucket for storing pipeline artifacts.\n",
    "        bucket_uri (str): URI of the bucket where inputs/outputs are stored.\n",
    "        client_name (str): Client name to personalize or segment the data/model.\n",
    "        region (str): Google Cloud region for deploying services and models.\n",
    "    \"\"\"\n",
    "\n",
    "    export_dataset_task = get_and_preprocess_dataset(\n",
    "        project_id=project_id,\n",
    "        client_name = client_name\n",
    "    ).set_display_name('Export Dataset')\n",
    "\n",
    "    training_task = model_training(\n",
    "        dataset=export_dataset_task.outputs[\"dataset\"],\n",
    "        client_name = client_name\n",
    "    ).set_display_name('Model Training')\n",
    "\n",
    "    model_deploy_task = deploy_model(\n",
    "        model=training_task.outputs[\"model\"],\n",
    "        project_id=project_id,\n",
    "        model_name= f\"{client_name}_precall_model\",\n",
    "        region = region,\n",
    "        client_name = client_name\n",
    "    ).set_display_name('Model Deployment')\n",
    "\n",
    "\n",
    "# Compile the pipeline function into a YAML definition\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pre_call_pipeline,\n",
    "    package_path=\"pre_call_training_pipeline.yaml\"\n",
    ")\n",
    "print(\"Pipeline has been compiled successfully.\")\n",
    "\n",
    "# Initialize the Google Cloud Storage client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Access the specific bucket\n",
    "bucket = storage_client.bucket(\"dev-bucket-ds-dashboard\")\n",
    "\n",
    "# Create a blob in the specified bucket directory\n",
    "blob = bucket.blob(\"yaml/pre_call_training_pipeline.yaml\")\n",
    "\n",
    "# Upload the compiled YAML file to Google Cloud Storage\n",
    "blob.upload_from_filename(\"pre_call_training_pipeline.yaml\")\n",
    "print(\"Pipeline YAML has been uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SrnA5tqng5j"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"{CLIENT_NAME}-pre-call-training-pipeline\",\n",
    "    template_path=\"pre_call_pipeline.yaml\",\n",
    "    pipeline_root= PIPELINE_ROOT,\n",
    "    location = REGION,\n",
    "    # parameter_values={\n",
    "    #     client_name = 'client'\n",
    "    # }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBE7/mL8qhX/+HMJoZYmQs",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
