{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvJD0Dbd4Ct2bR3k/zPj6k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xWnvB1yKl21O"},"outputs":[],"source":["# Specify the Google Cloud region where the services are located\n","REGION = \"asia-southeast2\"\n","\n","# Define the service account email used for Google Cloud services authentication\n","SERVICE_ACCOUNT = '118208205241-compute@developer.gserviceaccount.com'\n","\n","# Provide the service account's unique identifier number\n","SERVICE_ACCOUNT_NUMBER = '118208205241'\n","\n","# Execute the command to get the current Google Cloud project ID\n","project = !gcloud config get-value project\n","PROJECT_ID = project[0]\n","\n","# Print the retrieved project ID\n","print(\"Current Google Cloud Project ID:\", PROJECT_ID)"]},{"cell_type":"code","source":["# Define the client name associated with the operations\n","CLIENT_NAME = \"AKULAKU\"\n","\n","# Specify the URI of the Google Cloud Storage bucket\n","BUCKET_URI = 'gs://dev-bucket-ds-dashboard'\n","\n","# Extract and define the bucket name from the bucket URI\n","BUCKET_NAME = BUCKET_URI\n","\n","# Define the root directory for pipelines within the bucket, specific to the client\n","PIPELINE_ROOT = f\"{BUCKET_URI}/data_science/{CLIENT_NAME}\"\n","\n","# Define the full Google Cloud Storage bucket path for storing temporary data\n","GCS_BUCKET_NAME = 'gs://dev-bucket-ds-dashboard/data_science/temp_data_vertex/'\n","\n","# Define the full path to the dataset within the GCS bucket\n","GCS_PATH = 'gs://dev-bucket-ds-dashboard/data_science/temp_data_vertex/dataset.csv'\n","\n","# Specify the format of the training dataset\n","TRAINING_DATASET_FORMAT = \"csv\""],"metadata":{"id":"6qcvxb1bmTdU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import standard libraries\n","import pandas as pd\n","from typing import NamedTuple\n","\n","# Import Google Cloud client libraries\n","from google.cloud import bigquery, storage\n","import google.cloud.aiplatform as aiplatform\n","\n","# Import Kubeflow Pipeline components\n","import kfp\n","from kfp import compiler, dsl\n","from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component, OutputPath, InputPath\n","\n","# Import utilities for pipeline components\n","from google_cloud_pipeline_components.v1.custom_job import utils\n"],"metadata":{"id":"MZ0FNc09mWQD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the model ID to be used\n","model_id = '334339495774126080'\n","\n","# Create an AI Platform Model instance using the fully qualified model path\n","MODEL = aiplatform.Model(f\"projects/{SERVICE_ACCOUNT_NUMBER}/locations/{REGION}/models/{model_id}\")\n","\n","# Extract the full resource name of the model for easy reference\n","MODEL_RESOURCE_NAME = MODEL.resource_name\n","\n","# Print the model's resource name for verification\n","print(\"Model Resource Name:\", MODEL_RESOURCE_NAME)"],"metadata":{"id":"sZuQhR88mWUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the base path for all project and location based operations\n","PARENT = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n","\n","# Configure the API endpoint specific to the region for AI Platform\n","API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n","\n","# Set client options with the specific API endpoint\n","client_options = {\"api_endpoint\": API_ENDPOINT}\n","\n","# Initialize the AI Platform Job Service Client with the specified endpoint\n","CLIENT = aiplatform.gapic.JobServiceClient(client_options=client_options)\n","\n","# Print the API endpoint used for the client to verify the configuration\n","print(\"AI Platform API Endpoint:\", API_ENDPOINT)"],"metadata":{"id":"yHFkGrbcmWZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@component(\n","    packages_to_install=[\n","        \"google-cloud-bigquery[pandas]==3.10.0\",\n","        \"google-cloud-storage\",\n","        \"pandas\",\n","        \"joblib\",\n","        \"scikit-learn\",\n","    ],\n","    base_image = 'python:3.9'\n",")\n","def get_and_preprocess_dataset(\n","    project_id : str,\n","    dataset: Output[Dataset],\n","    client_name : str\n",") :\n","    \"\"\"\n","    Retrieves and preprocesses a dataset from BigQuery based on the project and client details,\n","    then applies encoding and dumps to a CSV.\n","\n","    Parameters:\n","        project_id (str): The Google Cloud project ID.\n","        dataset (Output[Dataset]): The dataset to be processed and stored.\n","        client_name (str): The client name to filter the data.\n","\n","    \"\"\"\n","\n","    import pandas as pd\n","    from sklearn.preprocessing import OrdinalEncoder\n","    from google.cloud import bigquery\n","\n","    def ensure_min_dpd(df):\n","        df.loc[df['dpd_number'] < 90, 'dpd_number'] = 90\n","        return df\n","\n","    def remove_zero_principal(df):\n","        return df[df['outstanding_principal_amount'] != 0]\n","\n","    def create_employment_dummies(df):\n","        employment_types = ['EMPLOYEE', 'ENTREPRENEUR', 'STUDENT', 'OTHERS', 'FREELANCE',\n","                            'GOVERNMENT EMPLOYEE', 'POLICE/MILITARY', 'UNEMPLOYED', 'UNKNOWN']\n","        df_employment = pd.get_dummies(df['employment_type_name'])\n","        df_employment = df_employment.reindex(columns=employment_types, fill_value=0)\n","        df_employment.columns = [f'employment_type_{col}' for col in df_employment.columns]\n","        return df_employment\n","\n","    def create_gender_dummies(df):\n","        gender = ['M', 'F']\n","        df_gender = pd.get_dummies(df['gender_name'])\n","        df_gender = df_gender.reindex(columns=gender, fill_value=0)\n","        df_gender.columns = [f'gender_name_{col}' for col in df_gender.columns]\n","        return df_gender\n","\n","    def concatenate_dataframes(df, df_employment, df_gender):\n","        return pd.concat([df, df_employment, df_gender], axis=1)\n","\n","    def drop_unnecessary_columns(df):\n","        drop_columns = ['external_loan_number', 'ktp_province_name', 'total_payment', 'age',\n","                        'campaign_valid_from', 'campaign_valid_to', 'campaign_name', 'client_name',\n","                        'employment_type_name', 'gender_name', 'Ever_Pay']\n","        return df.drop(columns=drop_columns)\n","\n","    def apply_ordinal_encoding(df, cols):\n","        encoder = OrdinalEncoder()\n","        df[cols] = encoder.fit_transform(df[cols])\n","        return df\n","\n","\n","    # Initialize BigQuery client\n","    client = bigquery.Client(project=project_id)\n","\n","    # Construct and execute a query to retrieve the dataset\n","    query = \"\"\"\n","    SELECT *\n","    FROM `prj-pav-id-vertexai-dev.data_science.ds_pre_call_predict`\n","    WHERE client_name = \"{0}\"\n","    \"\"\".format(client_name)\n","\n","    job_config = bigquery.QueryJobConfig()\n","    query_job = client.query(query=query, job_config=job_config)\n","    df = query_job.result().to_dataframe()\n","\n","    # Apply functions\n","    df = ensure_min_dpd(df)\n","    df = remove_zero_principal(df)\n","    df_employment = create_employment_dummies(df)\n","    df_gender = create_gender_dummies(df)\n","    df = concatenate_dataframes(df, df_employment, df_gender)\n","    df = drop_unnecessary_columns(df)\n","    df = apply_ordinal_encoding(df, ['dpd_number', 'outstanding_principal_amount'])\n","\n","    # Save the processed dataframe to a CSV file\n","    df.to_csv(dataset.path, index=False)"],"metadata":{"id":"Bi18jlGgmWcr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_NAME = \"pre_call_predict\"\n","\n","TRAINING_DATASET = 'bq://prj-pav-id-vertexai-dev.data_science.ds_pre_call'\n","\n","BQ_INPUT_URI = 'bq://prj-pav-id-vertexai-dev.data_science.temp_pre_call_batch_prediction'\n","BQ_OUTPUT_URI = 'bq://prj-pav-id-vertexai-dev.data_science'\n","\n","GS_OUTPUT_URI = 'gs://dev-bucket-ds-dashboard/data_science/batch_pred/'\n","\n","PRED_FORMAT = 'jsonl'\n","INS_FORMAT = 'jsonl'\n","TRAIN_DATA_FORMAT = 'bigquery'\n","\n","MACHINE_TYPE = \"n1-standard-4\"\n","EMAIL_ADDRESS = \"mdzeaulfath@id.pepper-advantage.com\""],"metadata":{"id":"_ChXexKvmhP7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decorator to specify packages and Python version for the environment\n","@component(\n","    packages_to_install=[\"google-cloud-aiplatform==1.25.0\"],\n","    base_image = 'python:3.9'\n",")\n","def create_kfp_batch_prediction_job(\n","    input_uri: Input[Dataset],\n","    output_uri: str,\n","    instances_format: str,\n","    predictions_format: str,\n","    job_name_prefix: str,\n","    model_name: str,\n","    deploy_compute: str,\n","    email_address: str,\n","    project_id: str,\n","    # region: str,\n","    training_dataset_format: str,\n","    training_dataset: str,\n","    location: str = \"asia-southeast2\",\n","    api_endpoint: str = \"asia-southeast2-aiplatform.googleapis.com\",\n","):\n","    \"\"\"\n","    Creates a batch prediction job for a machine learning model hosted on Google Cloud,\n","    configures its monitoring based on the project details provided.\n","\n","    Parameters:\n","        input_uri (Input[Dataset]): The location of input data for prediction.\n","        output_uri (str): The Cloud Storage bucket for saving predictions.\n","        instances_format (str): Format of the input instances.\n","        predictions_format (str): Format of the output predictions.\n","        job_name_prefix (str): Prefix for the job name.\n","        model_name (str): Name of the model to be used.\n","        deploy_compute (str): Machine type for the prediction.\n","        email_address (str): Email address for alert configurations.\n","        project_id (str): Google Cloud project ID.\n","        training_dataset_format (str): Format of the training dataset.\n","        training_dataset (str): Location of the training dataset.\n","        location (str): Location of the AI platform services.\n","        api_endpoint (str): API endpoint for the job service.\n","\n","    \"\"\"\n","\n","    # Import necessary libraries from google.cloud.aiplatform_v1beta1\n","    from google.cloud.aiplatform_v1beta1.types import (\n","        BatchDedicatedResources, BatchPredictionJob, GcsDestination, GcsSource,\n","        MachineSpec, ModelMonitoringAlertConfig,\n","        ModelMonitoringObjectiveConfig, ThresholdConfig, BigQuerySource, GcsDestination,\n","        ModelMonitoringConfig, CreateBatchPredictionJobRequest, BigQueryDestination\n","    )\n","    from google.cloud.aiplatform_v1beta1.services.job_service import JobServiceClient\n","\n","    # Set client options and initialize the JobServiceClient\n","    client_options = {\"api_endpoint\": api_endpoint}\n","    client = JobServiceClient(client_options=client_options)\n","\n","    # Generate batch prediction job name\n","    BATCH_PREDICTION_JOB_NAME = f\"{job_name_prefix}_kfp\"\n","\n","    # Define the BatchPredictionJob\n","    batch_prediction_job = BatchPredictionJob(\n","        display_name=BATCH_PREDICTION_JOB_NAME,\n","        model=model_name,\n","        input_config=BatchPredictionJob.InputConfig(\n","            instances_format=instances_format,\n","            gcs_source=GcsSource(uris=[input_uri.uri]),\n","        ),\n","        output_config=BatchPredictionJob.OutputConfig(\n","            predictions_format=predictions_format,\n","            gcs_destination=GcsDestination(output_uri_prefix=output_uri),\n","            # bigquery_destination = BigQueryDestination(output_uri=output_uri)\n","        ),\n","        dedicated_resources=BatchDedicatedResources(\n","            machine_spec=MachineSpec(machine_type=deploy_compute),\n","            starting_replica_count=1,\n","            max_replica_count=1,\n","        ),\n","       model_monitoring_config=ModelMonitoringConfig(\n","            alert_config=ModelMonitoringAlertConfig(\n","            email_alert_config=ModelMonitoringAlertConfig.EmailAlertConfig(\n","                user_emails=[email_address])\n","            ),\n","            objective_configs=[\n","\n","                ModelMonitoringObjectiveConfig(\n","                    training_dataset=ModelMonitoringObjectiveConfig.TrainingDataset(\n","                        data_format=training_dataset_format,\n","                        bigquery_source=BigQuerySource(input_uri=training_dataset),\n","                    ),\n","                    prediction_drift_detection_config=ModelMonitoringObjectiveConfig.PredictionDriftDetectionConfig(\n","                        default_drift_threshold= {\n","                            \"value\": 0.3\n","                          }\n","                    ),\n","                )\n","            ],\n","        ),\n","    )\n","\n","    # Construct the request and create the batch prediction job\n","    parent = f\"projects/{project_id}/locations/{location}\"\n","    request = CreateBatchPredictionJobRequest(\n","        parent=parent, batch_prediction_job=batch_prediction_job\n","    )\n","\n","    # Make the request\n","    response = client.create_batch_prediction_job(request=request)\n","    print(\"response:\", response)"],"metadata":{"id":"IRrSpP4xmjlL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decorator to specify packages and Python version for the environment\n","@component(\n","    packages_to_install=[\n","        \"google-cloud-bigquery[pandas]==3.10.0\",\n","        \"google-cloud-storage\",\n","        \"pandas\",\n","    ],\n","    base_image = 'python:3.9'\n",")\n","def get_agent_data(\n","    project_id : str,\n","    dataset: Output[Dataset],\n",") :\n","    \"\"\"\n","    Fetches data about agents from a BigQuery dataset and writes it to a CSV file.\n","\n","    Parameters:\n","        project_id (str): The Google Cloud project ID where the dataset is hosted.\n","        dataset (Output[Dataset]): The output location where the CSV file will be saved.\n","    \"\"\"\n","\n","    # Import necessary libraries\n","    import pandas as pd\n","    from google.cloud import bigquery\n","\n","    # Initialize BigQuery client\n","    client = bigquery.Client(project=project_id)\n","\n","    # SQL query to select distinct agent names from the specified table\n","    query = \"\"\"\n","        SELECT DISTINCT assigned_agent\n","    FROM `prj-pav-id-vertexai-dev.data_science.after_call_agent_handle_pay_probability`\n","        \"\"\"\n","    # Configure the query job\n","    job_config = bigquery.QueryJobConfig()\n","\n","    # Execute the query\n","    query_job = client.query(query=query, job_config=job_config)\n","\n","    # Convert the result to a DataFrame\n","    df_agent = query_job.result().to_dataframe()\n","\n","    # Write the DataFrame to a CSV file without the index\n","    df_agent.to_csv(dataset.path, index=False)"],"metadata":{"id":"rUYc4IMJml0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@component(\n","    packages_to_install=[\n","        \"pandas\",\n","        \"joblib\",\n","        \"scikit-learn\",\n","        \"google-cloud-bigquery[pandas]==3.10.0\",\n","        \"google-cloud-storage\",\n","        \"google-cloud-aiplatform==1.25.0\",\n","        \"xgboost\",\n","        \"gcsfs\",\n","        \"pandas-gbq\"\n","    ],\n","    base_image = 'python:3.9'\n",")\n","def write_predicted_data(\n","    project_id : str,\n","    data_input : Input[Dataset],\n","    data_output : Output[Dataset],\n","    region: str,\n","    client_name : str,\n","):\n","\n","    \"\"\"\n","    Fetches prediction data from BigQuery, preprocesses it, applies a predictive model,\n","    and then exports the results to a CSV file.\n","\n","    Parameters:\n","        project_id (str): Google Cloud project ID.\n","        data_input (Input[Dataset]): Input dataset for model predictions.\n","        data_output (Output[Dataset]): Output path for the predicted data.\n","        region (str): Google Cloud region of the project.\n","        client_name (str): Client's name to filter and model data.\n","    \"\"\"\n","\n","    # Import necessary libraries\n","    import os\n","    import pandas as pd\n","    import joblib\n","    import numpy as np\n","    import tempfile\n","    from google.cloud import bigquery, storage, aiplatform\n","    from sklearn.preprocessing import OrdinalEncoder\n","    from xgboost import XGBClassifier\n","\n","    def ensure_min_dpd(df):\n","        df.loc[df['dpd_number'] < 90, 'dpd_number'] = 90\n","        return df\n","\n","    def remove_zero_principal(df):\n","        return df[df['outstanding_principal_amount'] != 0]\n","\n","    def create_employment_dummies(df):\n","        employment_types = ['EMPLOYEE', 'ENTREPRENEUR', 'STUDENT', 'OTHERS', 'FREELANCE',\n","                            'GOVERNMENT EMPLOYEE', 'POLICE/MILITARY', 'UNEMPLOYED', 'UNKNOWN']\n","        df_employment = pd.get_dummies(df['employment_type_name'])\n","        df_employment = df_employment.reindex(columns=employment_types, fill_value=0)\n","        df_employment.columns = [f'employment_type_{col}' for col in df_employment.columns]\n","        return df_employment\n","\n","    def create_gender_dummies(df):\n","        gender = ['M', 'F']\n","        df_gender = pd.get_dummies(df['gender_name'])\n","        df_gender = df_gender.reindex(columns=gender, fill_value=0)\n","        df_gender.columns = [f'gender_name_{col}' for col in df_gender.columns]\n","        return df_gender\n","\n","    def concatenate_dataframes(df, df_employment, df_gender):\n","        return pd.concat([df, df_employment, df_gender], axis=1)\n","\n","    def drop_unnecessary_columns(df):\n","        drop_columns = ['external_loan_number', 'ktp_province_name', 'total_payment', 'age',\n","                        'campaign_valid_from', 'campaign_valid_to', 'campaign_name', 'client_name',\n","                        'employment_type_name', 'gender_name', 'Ever_Pay']\n","        return df.drop(columns=drop_columns)\n","\n","    def apply_ordinal_encoding(df, cols):\n","        encoder = OrdinalEncoder()\n","        df[cols] = encoder.fit_transform(df[cols])\n","        return df\n","\n","    def save_to_csv(df, path):\n","        df.to_csv(path, index=False)\n","\n","    def load_model_from_gcs(model_name, project_id, region):\n","        listed_models = aiplatform.Model.list(filter= f'display_name=\"{model_name}\"', project=project_id,location= region)\n","        modelpath_auto = listed_models[0].uri + '/model.joblib'\n","        client = storage.Client()\n","        bucket_name, blob_name = modelpath_auto.replace(\"gs://\", \"\").split(\"/\", 1)\n","        bucket = client.get_bucket(bucket_name)\n","        blob = bucket.blob(blob_name)\n","\n","        with tempfile.NamedTemporaryFile() as temp_file:\n","            blob.download_to_filename(temp_file.name)\n","            loaded_model = joblib.load(temp_file.name)\n","        return loaded_model\n","\n","    def make_predictions(df, model):\n","        predictions = model.predict(df)\n","        probabilities = model.predict_proba(df)\n","        df['prediction'] = predictions\n","        df['probability_not_pay'] = probabilities[:, 0]\n","        df['probability_pay'] = probabilities[:, 1]\n","        df['prediction'] = df['prediction'].map({1: 'pay', 0: 'not_pay'})\n","        return df\n","\n","    def add_back_columns(df_original, df_modified, columns_to_add):\n","        df_modified[columns_to_add] = df_original[columns_to_add]\n","        return df_modified\n","\n","    def assign_agents(df, df_agent):\n","        df['group_id'] = df.groupby('external_loan_number').ngroup()\n","        num_agents = len(df_agent)\n","        df['assigned_agent'] = df['group_id'].apply(lambda x: df_agent['assigned_agent'][x % num_agents])\n","        df.drop(columns='group_id', axis=1, inplace=True)\n","        return df\n","\n","\n","    # Initialize BigQuery client\n","    client = bigquery.Client(project=project_id)\n","\n","    # Construct and execute a query to retrieve the dataset\n","    query = \"\"\"\n","    SELECT *\n","    FROM `prj-pav-id-vertexai-dev.data_science.ds_pre_call_predict`\n","    WHERE client_name = \"{0}\"\n","    \"\"\".format(client_name)\n","\n","    job_config = bigquery.QueryJobConfig()\n","    query_job = client.query(query=query, job_config=job_config)\n","    df_original = query_job.result().to_dataframe()\n","\n","    query_agent = \"\"\"\n","    SELECT DISTINCT assigned_agent\n","    FROM `prj-pav-id-vertexai-dev.data_science.after_call_agent_handle_pay_probability`\n","    \"\"\"\n","    job_config = bigquery.QueryJobConfig()\n","    query_job = client.query(query=query_agent, job_config=job_config)\n","    df_agent = query_job.result().to_dataframe()\n","\n","    # Apply functions\n","    df = ensure_min_dpd(df_original)\n","    df = remove_zero_principal(df)\n","    df_employment = create_employment_dummies(df)\n","    df_gender = create_gender_dummies(df)\n","    df = concatenate_dataframes(df, df_employment, df_gender)\n","    df = drop_unnecessary_columns(df)\n","    df = apply_ordinal_encoding(df, ['dpd_number', 'outstanding_principal_amount'])\n","\n","    model = load_model_from_gcs(f'{client_name}_precall_model', project_id, region)\n","    df_pred = make_predictions(df, model)\n","\n","    df_pred_with_eln = add_back_columns(df_original=df_original, df_modified=df_pred, columns_to_add=['external_loan_number'])\n","    df_result = assign_agents(df_pred_with_eln, df_agent)\n","\n","    # Output the data to a CSV file\n","    df_result.to_csv(data_output.path, index=False)"],"metadata":{"id":"u4Y5kqk7mmsT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TABLE_ID= \"prj-pav-id-vertexai-dev.data_science.pre_call_result\""],"metadata":{"id":"IoeXkUIcmqL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@component(\n","    packages_to_install=[\n","        \"pandas\",\n","        \"pandas_gbq\",\n","        \"google-cloud-bigquery[pandas]\",\n","        \"google-cloud-bigquery==3.10.0\",\n","        \"fsspec\",\n","        \"gcsfs\"\n","    ],\n","    base_image='python:3.9'\n",")\n","def write_to_bigquery(\n","    project_id: str,\n","    table_id: str,\n","    input_uri: Input[Dataset],\n","):\n","    \"\"\"\n","    Reads data from a CSV file, checks for duplicate column names, and writes it to a BigQuery table.\n","\n","    Parameters:\n","        project_id (str): Google Cloud project ID where the BigQuery table is located.\n","        table_id (str): BigQuery table ID to which the data will be appended.\n","        input_uri (Input[Dataset]): URI of the input dataset in CSV format.\n","    \"\"\"\n","\n","    # Import necessary libraries\n","    import pandas as pd\n","    import pandas_gbq as pd_gbq\n","    from google.cloud import bigquery\n","\n","    # Function to check for duplicate column names in the DataFrame\n","    def check_duplicate_columns(df):\n","        column_series = pd.Series(df.columns)\n","        duplicate_names = column_series[column_series.duplicated()]\n","        if not duplicate_names.empty:\n","            return False, duplicate_names.values\n","        return True, None\n","\n","    # Read data from CSV with specific data types for each column\n","    df = pd.read_csv(input_uri.uri, index_col=None, dtype={\n","        'external_loan_number': 'Int64',\n","        'dpd_number': 'Int64',\n","        'client_name': 'object',\n","        'campaign_name': 'object',\n","        'outstanding_principal_amount': 'float64',\n","        'age': 'object',\n","        'has_age': 'Int64',\n","        'gender_name': 'object',\n","        'has_gender': 'Int64',\n","        'has_email': 'Int64',\n","        'ktp_province_name': 'object',\n","        'has_ktp_province_name': 'Int64',\n","        'has_address': 'Int64',\n","        'mobile_phone_number_exist': 'Int64',\n","        'emergency_phone_number_exist': 'Int64',\n","        'good_contact': 'Int64',\n","        'complete_demography': 'Int64',\n","        'complete_information': 'Int64',\n","        'Ever_Pay': 'Int64',\n","        'total_payment': 'float64',\n","        'prediction': 'object',\n","        'probability_not_pay': 'float64',\n","        'probability_pay': 'float64',\n","        'employment_type_name': 'object',\n","        'has_employment_type_name': 'Int64',\n","        'assigned_agent': 'object'\n","    })\n","\n","    # Check for duplicate columns\n","    is_valid, duplicates = check_duplicate_columns(df)\n","    if is_valid:\n","        # If no duplicates, upload data to BigQuery\n","        pd_gbq.to_gbq(df, table_id, project_id=project_id, if_exists='append')\n","        print(\"Data uploaded successfully to BigQuery.\")\n","    else:\n","        # If duplicates exist, display an error message\n","        print(\"Duplicated columns, please fix it first:\", duplicates)"],"metadata":{"id":"_kVzMQa0msYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define pipeline\n","@dsl.pipeline(\n","    name=\"precall-batchpred-pipeline\",\n","    pipeline_root = PIPELINE_ROOT\n",")\n","def pre_call_pipeline(\n","    project_id: str = PROJECT_ID ,\n","    bucket_name: str = BUCKET_NAME,\n","    bucket_uri: str = BUCKET_URI,\n","    client_name: str = CLIENT_NAME,\n","    region: str = REGION\n","):\n","    \"\"\"\n","    Defines a Kubeflow Pipeline for preprocessing data, making batch predictions,\n","    getting agent data, and writing predictions and results to BigQuery.\n","\n","    Parameters:\n","        project_id (str): GCP project ID.\n","        bucket_name (str): Name of the GCP storage bucket.\n","        bucket_uri (str): URI of the GCP storage bucket.\n","        client_name (str): Name of the client to filter data.\n","        region (str): GCP region.\n","    \"\"\"\n","\n","\n","    # Task to export agent dataset\n","    get_agent_data_task = get_agent_data(\n","        project_id=project_id,\n","    ).set_display_name('Export Agent Dataset')\n","\n","    # Task to write predicted data, occurs after preprocessing task\n","    write_predicted_data_task = write_predicted_data(\n","        project_id=project_id,\n","        # model_input = getmodel_op.outputs['model_path'],\n","        data_input = get_agent_data_task.outputs['dataset'],\n","        region = REGION,\n","        client_name = client_name,\n","        # input_bucket_name_path= getmodel_op.outputs['bucket_name_path'],\n","        # input_blob_name_path= getmodel_op.outputs['blob_name_path'],\n","    ).set_display_name('Prediction Result')\n","\n","    # Task to write results to BigQuery\n","    write_to_bigquery_task = write_to_bigquery(\n","        project_id= project_id,\n","        table_id= TABLE_ID,\n","        input_uri= write_predicted_data_task.outputs['data_output']\n","    ).set_display_name('Upload to BigQuery')\n","\n","# Compile the pipeline function into a YAML definition\n","compiler.Compiler().compile(\n","    pipeline_func=pre_call_pipeline,\n","    package_path=\"pre_call_batch_pred_pipeline.yaml\"\n",")\n","print(\"Pipeline has been compiled successfully.\")\n","\n","# Initialize the Google Cloud Storage client\n","storage_client = storage.Client()\n","\n","# Access the specific bucket\n","bucket = storage_client.bucket(\"dev-bucket-ds-dashboard\")\n","\n","# Create a blob in the specified bucket directory\n","blob = bucket.blob(\"yaml/pre_call_batch_pred_pipeline.yaml\")\n","\n","# Upload the compiled YAML file to Google Cloud Storage\n","blob.upload_from_filename(\"pre_call_batch_pred_pipeline.yaml\")\n","print(\"Pipeline YAML has been uploaded successfully.\")"],"metadata":{"id":"muCLJappmuoq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a PipelineJob instance with the specified parameters\n","job = aiplatform.PipelineJob(\n","    display_name=\"pre-call-batchpred-pipeline\",\n","    template_path=\"pre_call_batch_pred_pipeline.yaml\",\n","    pipeline_root= PIPELINE_ROOT,\n","    location = REGION,\n",")\n","\n","# Execute the pipeline job\n","job.run()\n","print(\"Pipeline job has been initiated.\")"],"metadata":{"id":"QjFHKIrcmxE6"},"execution_count":null,"outputs":[]}]}